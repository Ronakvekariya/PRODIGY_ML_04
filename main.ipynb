{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1620afa-9ef5-4d99-becf-eda2dbcbba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Dense , MaxPooling2D , Dropout , Conv2D , BatchNormalization , Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7457c846-3959-4705-ae00-348424ffdd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "images_palm = []\n",
    "images_i = []\n",
    "images_fist = []\n",
    "images_fist_moved = []\n",
    "images_palm_moved = []\n",
    "images_c = []\n",
    "images_down = []\n",
    "images_ok = []\n",
    "images_index = []\n",
    "images_thumb = []\n",
    "\n",
    "def read_all_images(directory , images):\n",
    "    if os.path.exists(directory):\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.lower().endswith(('png', 'jpg', 'jpeg', 'tiff', 'bmp', 'gif')):\n",
    "                image_path = os.path.join(directory, filename)\n",
    "                img = image.load_img(image_path , target_size=(128, 128, 1), color_mode='grayscale')\n",
    "                img = image.img_to_array(img)\n",
    "                img = img / 255\n",
    "                images.append(img)\n",
    "    else:\n",
    "        print(f\"The directory {directory} does not exist.\")\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fa6504f-e617-4e53-b935-0b62a829b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,7):\n",
    "    directory = './inages/' + '0' + str(i) + '/01_palm'\n",
    "    read_all_images(directory , images_palm)\n",
    "for i in range(0,7):\n",
    "    directory = './inages/' + '0' + str(i) + '/02_l'\n",
    "    read_all_images(directory , images_i)\n",
    "for i in range(0,7):\n",
    "    directory = './inages/' + '0' + str(i) + '/03_fist'\n",
    "    read_all_images(directory , images_fist)\n",
    "for i in range(0,7):\n",
    "    directory = './inages/' + '0' + str(i) + '/04_fist_moved'\n",
    "    read_all_images(directory , images_fist_moved)\n",
    "for i in range(0,7):\n",
    "    directory = './inages/' + '0' + str(i) + '/05_thumb'\n",
    "    read_all_images(directory , images_thumb)\n",
    "for i in range(0,7):\n",
    "    directory = './inages/' + '0' + str(i) + '/06_index'\n",
    "    read_all_images(directory , images_index)\n",
    "for i in range(0,7):\n",
    "    directory = './inages/' + '0' + str(i) + '/07_ok'\n",
    "    read_all_images(directory , images_ok)\n",
    "for i in range(0,7):\n",
    "    directory = './inages/' + '0' + str(i) + '/08_palm_moved'\n",
    "    read_all_images(directory , images_palm_moved)\n",
    "for i in range(0,7):\n",
    "    directory = './inages/' + '0' + str(i) + '/09_c'\n",
    "    read_all_images(directory , images_c)\n",
    "for i in range(0,7):\n",
    "    directory = './inages/' + '0' + str(i) + '/10_down'\n",
    "    read_all_images(directory , images_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7a1a288-bed1-4a0f-b7e6-db5d359bfd74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "value = 0\n",
    "label_palm = [value]*len(images_palm)\n",
    "value = 1 \n",
    "label_i = [value]*len(images_i)\n",
    "value = 2\n",
    "label_first = [value]*len(images_fist)\n",
    "value = 3\n",
    "label_first_moved = [value]*len(images_fist_moved)\n",
    "value = 4\n",
    "label_thumb = [value]*len(images_thumb)\n",
    "value = 5 \n",
    "label_index = [value]*len(images_index)\n",
    "value = 6 \n",
    "label_ok = [value]*len(images_ok)\n",
    "value = 7 \n",
    "label_palm_moved = [value]*len(images_palm_moved)\n",
    "value = 8 \n",
    "label_c = [value]*len(images_c)\n",
    "value = 9 \n",
    "label_down = [value]*len(images_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d62cdbc-e7d6-4901-b436-0e2aa97e0842",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list =images_palm + images_i + images_fist + images_fist_moved + images_palm_moved + images_c +images_down + images_ok +images_index +images_thumb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "432f1e41-53af-424c-8fe1-1c97a30e40ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list =  label_palm + label_i + label_first + label_first_moved + label_palm_moved + label_c + label_down + label_ok + label_index + label_thumb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f266d06f-4a04-490b-9ebe-70098ae24138",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array = np.array(image_list)\n",
    "label_array = np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8928c73d-ab74-4928-b8fa-57e862a043df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3af163eb-36d9-41b5-80ab-4d88b7e7200a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b072cc08-386b-4094-89c7-52c48b291015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c48797ce-6f75-4b62-b4c6-e4a8a574008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93202cb6-8e14-4213-bad5-331cf5ef3d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_val , train_out , val_out = train_test_split(image_array , label_array , test_size = 0.2 , random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e3a8df0-7936-485b-910d-7fa8c2fd1917",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(np.float16)\n",
    "x_val = x_val.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f4af1ce-993d-470d-8df5-fd1be60aaa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b544a8c-920f-456d-a586-45c44614f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out = to_categorical(train_out)\n",
    "val_out = to_categorical(val_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5240c546-1924-4dbb-8890-aec89fcb1d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 644ms/step - accuracy: 0.6808 - loss: 3.1270 - val_accuracy: 0.0921 - val_loss: 11.4926\n",
      "Epoch 2/10\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 575ms/step - accuracy: 0.9791 - loss: 0.0835 - val_accuracy: 0.9654 - val_loss: 0.1039\n",
      "Epoch 3/10\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 550ms/step - accuracy: 0.9937 - loss: 0.0236 - val_accuracy: 1.0000 - val_loss: 2.4295e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 552ms/step - accuracy: 0.9972 - loss: 0.0095 - val_accuracy: 1.0000 - val_loss: 2.3812e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 562ms/step - accuracy: 0.9987 - loss: 0.0038 - val_accuracy: 0.9996 - val_loss: 6.5449e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 565ms/step - accuracy: 0.9995 - loss: 0.0017 - val_accuracy: 0.9993 - val_loss: 0.0015\n",
      "Epoch 7/10\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 600ms/step - accuracy: 0.9994 - loss: 0.0011 - val_accuracy: 0.9989 - val_loss: 0.0036\n",
      "Epoch 8/10\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 599ms/step - accuracy: 0.9993 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 9.5126e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 598ms/step - accuracy: 0.9999 - loss: 6.2175e-04 - val_accuracy: 1.0000 - val_loss: 7.0340e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 578ms/step - accuracy: 0.9999 - loss: 5.2940e-04 - val_accuracy: 1.0000 - val_loss: 3.5920e-04\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3,3) , strides = 1,padding = 'same', input_shape = (128,128,1),activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(4,4) , strides = 1,padding = 'same' , activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(1024 , activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "    model.compile(optimizer = tf.keras.optimizers.RMSprop(0.0001), # 1e-4\n",
    "              loss = 'categorical_crossentropy', # Ideal for multiclass tasks\n",
    "              metrics = ['accuracy']) # Evaluation metric\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor = 'val_accuracy',\n",
    "                              patience = 5, mode = 'max',\n",
    "                              restore_best_weights = True)\n",
    "\n",
    "    checkpoint = ModelCheckpoint('best_model.keras',\n",
    "                            monitor = 'val_accuracy',\n",
    "                            save_best_only = True)\n",
    "    model.fit(x_train , train_out , epochs=10 , validation_data=(x_val , val_out))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e734e9a2-4cda-4561-96c2-0ea1ab5c815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"cnn_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bae7e681-f767-4c3e-b41a-53785d9b47f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('image_array.npy' , image_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26f38e8c-2b1e-4469-8a76-8d23a7e9b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('label_array.npy' , label_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb9734-07aa-4b63-9862-21649f6ac48d",
   "metadata": {},
   "source": [
    "testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1e19dd5-d0ba-4330-8647-b05442d50ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_palm = []\n",
    "test_images_i = []\n",
    "test_images_fist = []\n",
    "test_images_fist_moved = []\n",
    "test_images_thumb = []\n",
    "test_images_index = []\n",
    "test_images_down = []\n",
    "test_images_c = []\n",
    "test_images_palm_moved = []\n",
    "test_images_ok = []\n",
    "\n",
    "for i in range(8,9):\n",
    "    directory = './inages/' + '0' + str(i) + '/01_palm'\n",
    "    read_all_images(directory , test_images_palm)\n",
    "for i in range(8,9):\n",
    "    directory = './inages/' + '0' + str(i) + '/02_l'\n",
    "    read_all_images(directory , test_images_i)\n",
    "for i in range(8,9):\n",
    "    directory = './inages/' + '0' + str(i) + '/03_fist'\n",
    "    read_all_images(directory , test_images_fist)\n",
    "for i in range(8,9):\n",
    "    directory = './inages/' + '0' + str(i) + '/04_fist_moved'\n",
    "    read_all_images(directory , test_images_fist_moved)\n",
    "for i in range(8,9):\n",
    "    directory = './inages/' + '0' + str(i) + '/05_thumb'\n",
    "    read_all_images(directory , test_images_thumb)\n",
    "for i in range(8,9):\n",
    "    directory = './inages/' + '0' + str(i) + '/06_index'\n",
    "    read_all_images(directory , test_images_index)\n",
    "for i in range(8,9):\n",
    "    directory = './inages/' + '0' + str(i) + '/07_ok'\n",
    "    read_all_images(directory , test_images_ok)\n",
    "for i in range(8,9):\n",
    "    directory = './inages/' + '0' + str(i) + '/08_palm_moved'\n",
    "    read_all_images(directory , test_images_palm_moved)\n",
    "for i in range(8,9):\n",
    "    directory = './inages/' + '0' + str(i) + '/09_c'\n",
    "    read_all_images(directory , test_images_c)\n",
    "for i in range(8,9):\n",
    "    directory = './inages/' + '0' + str(i) + '/10_down'\n",
    "    read_all_images(directory , test_images_down)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8112b0f-b3c2-432f-b75e-1e9469e7794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 0\n",
    "test_label_palm = [value]*len(test_images_palm)\n",
    "value = 1 \n",
    "test_label_i = [value]*len(test_images_i)\n",
    "value = 2\n",
    "test_label_first = [value]*len(test_images_fist)\n",
    "value = 3\n",
    "test_label_first_moved = [value]*len(test_images_fist_moved)\n",
    "value = 4\n",
    "test_label_thumb = [value]*len(test_images_thumb)\n",
    "value = 5 \n",
    "test_label_index = [value]*len(test_images_index)\n",
    "value = 6 \n",
    "test_label_ok = [value]*len(test_images_ok)\n",
    "value = 7 \n",
    "test_label_palm_moved = [value]*len(test_images_palm_moved)\n",
    "value = 8 \n",
    "test_label_c = [value]*len(test_images_c)\n",
    "value = 9 \n",
    "test_label_down = [value]*len(test_images_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "690b97bd-585a-4603-8aa8-1317795831b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_list =  test_label_palm + test_label_i + test_label_first + test_label_first_moved + test_label_palm_moved + test_label_c + test_label_down + test_label_ok + test_label_index + test_label_thumb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89145a8b-605d-4392-ae34-a31856f489fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_list =test_images_palm + test_images_i + test_images_fist + test_images_fist_moved + test_images_palm_moved + test_images_c +test_images_down + test_images_ok +test_images_index +test_images_thumb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b7b781c-1667-4d95-952a-65647e06135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_array = np.array(test_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "527cfc87-424b-45fd-95af-81636b3160fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 110ms/step\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict(test_image_array)\n",
    "pred_classes = np.argmax(predict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f52bb6e4-1dc9-4986-8819-aa44b99f63af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae8a37b0-4f9a-4aa7-9c66-2cfd67f2cf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.50      0.40       200\n",
      "           1       0.70      0.67      0.68       200\n",
      "           2       0.69      0.30      0.42       200\n",
      "           3       0.00      0.00      0.00       200\n",
      "           4       0.42      0.12      0.19       200\n",
      "           5       0.44      0.72      0.55       200\n",
      "           6       0.11      0.02      0.03       200\n",
      "           7       0.47      1.00      0.64       200\n",
      "           8       0.38      0.35      0.36       200\n",
      "           9       0.56      1.00      0.72       200\n",
      "\n",
      "    accuracy                           0.47      2000\n",
      "   macro avg       0.41      0.47      0.40      2000\n",
      "weighted avg       0.41      0.47      0.40      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[100   0  11  13  14  12   7  11   2  30]\n",
      " [  6 133  17  10  10   2   0   6   3  13]\n",
      " [ 20   0  61   7   9  36  24   0  19  24]\n",
      " [ 84   0   0   0   0   8   0  16  92   0]\n",
      " [ 80   0   0   0  24  96   0   0   0   0]\n",
      " [  0  56   0   0   0 144   0   0   0   0]\n",
      " [  0   0   0   0   0   0   4 196   0   0]\n",
      " [  0   0   0   0   0   0   0 200   0   0]\n",
      " [ 14   0   0   0   0  27   0   0  70  89]\n",
      " [  0   0   0   0   0   0   0   0   0 200]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_label_list, pred_classes))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(test_label_list, pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ad62d-18db-4091-a636-c4fc918948fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
